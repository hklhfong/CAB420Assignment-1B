# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1niBRKXab_9x1fCmDsAqyRql3t1rJQvXa
"""

import random
import numpy
import numpy.matlib
import scipy
import matplotlib.pyplot as plt

import os
import glob
import cv2

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import backend as K

from sklearn import decomposition
from sklearn import discriminant_analysis
from sklearn import datasets
from sklearn.manifold import TSNE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import plot_confusion_matrix

test_a = r'C:\Users\user\Downloads\Assignment_1B_Data\Data\Q1\VIPeR\Test\cam_a\*.bmp'
test_b = r'C:\Users\user\Downloads\Assignment_1B_Data\Data\Q1\VIPeR\Test\cam_b\*.bmp'
train_a = r"C:\Users\user\Downloads\Assignment_1B_Data\Data\Q1\VIPeR\Train\cam_a\*.bmp"
train_b = r"C:\Users\user\Downloads\Assignment_1B_Data\Data\Q1\VIPeR\Train\cam_b\*.bmp"


files = glob.glob(test_a)
files.sort()

data_test_a = []
for f in files:
    d = {}
    head, tail = os.path.split(f)
    parts = tail.split('_')
    if (len(parts) == 2):
        d['label'] = int(parts[0])
        d['image'] = cv2.imread(f)
        data_test_a.append(d)
    else:
        print('Could not load: ' + f + '! Incorrectly formatted filename')

files = glob.glob(test_b)
files.sort()

data_test_b = []
for f in files:
    d = {}
    head, tail = os.path.split(f)
    parts = tail.split('_')
    if (len(parts) == 2):
        d['label'] = int(parts[0])
        d['image'] = cv2.imread(f)
        data_test_b.append(d)
    else:
        print('Could not load: ' + f + '! Incorrectly formatted filename')

files = glob.glob(train_a)
files.sort()

data_train_a = []
for f in files:
    d = {}
    head, tail = os.path.split(f)
    parts = tail.split('_')
    if (len(parts) == 2):
        d['label'] = int(parts[0])
        d['image'] = cv2.imread(f)
        data_train_a.append(d)
    else:
        print('Could not load: ' + f + '! Incorrectly formatted filename')

files = glob.glob(train_b)
files.sort()

data_train_b = []
for f in files:
    d = {}
    head, tail = os.path.split(f)
    parts = tail.split('_')
    if (len(parts) == 2):
        d['label'] = int(parts[0])
        d['image'] = cv2.imread(f)
        data_train_b.append(d)
    else:
        print('Could not load: ' + f + '! Incorrectly formatted filename')

test_a_X = numpy.array([d['image'] for d in data_test_a])/255
test_a_Y = numpy.array([d['label'] for d in data_test_a])

test_b_X = numpy.array([d['image'] for d in data_test_b])/255
test_b_Y = numpy.array([d['label'] for d in data_test_b])

test_X = numpy.concatenate((test_a_X, test_b_X), axis=0)
test_Y = numpy.concatenate((test_a_Y, test_b_Y), axis=0)

train_a_X = numpy.array([d['image'] for d in data_train_a])/255
train_a_Y = numpy.array([d['label'] for d in data_train_a])

train_b_X = numpy.array([d['image'] for d in data_train_b])/255
train_b_Y = numpy.array([d['label'] for d in data_train_b])

train_X = numpy.concatenate((train_a_X, train_b_X), axis=0)
train_Y = numpy.concatenate((train_a_Y, train_b_Y), axis=0)

print(train_X.shape)
fig = plt.figure(figsize=[10, 8])
for i in range(10):
    ax = fig.add_subplot(2, 10, i + 1)
    ax.imshow(train_X[i,:,:,0])
for i in range(10):
     ax = fig.add_subplot(2, 10, i + 11)
     ax.imshow(test_X[i,:,:,0])

"""# Part 1"""

print(numpy.min(train_a_X),numpy.max(train_a_X))
train_a_X = train_a_X/255.0
train_b_X = train_b_X/255.0
test_a_X = test_a_X/255.0
test_b_X = test_b_X/255.0
print(numpy.min(train_a_X),numpy.max(train_a_X))
print(train_a_X.shape)

nsamples, nx, ny, nz = train_a_X.shape
train_a_X_reshape = train_a_X.reshape((nsamples,nx*ny*nz))

nsamples, nx, ny, nz = train_b_X.shape
train_b_X_reshape = train_b_X.reshape((nsamples,nx*ny*nz))

nsamples, nx, ny, nz = test_a_X.shape
test_a_X_reshape = test_a_X.reshape((nsamples,nx*ny*nz))

nsamples, nx, ny, nz = test_b_X.shape
test_b_X_reshape = test_b_X.reshape((nsamples,nx*ny*nz))

pca_a = decomposition.PCA()
pca_a.fit(train_a_X_reshape)

pca_b = decomposition.PCA()
pca_b.fit(train_b_X_reshape)


print(pca_a.explained_variance_ratio_)
print(pca_b.explained_variance_ratio_)

transformed_train_a = pca_a.transform(train_a_X_reshape)
transformed_train_b = pca_b.transform(train_b_X_reshape)
transformed_test_a = pca_a.transform(test_a_X_reshape)
transformed_test_b = pca_b.transform(test_b_X_reshape)

cumulative_sum = numpy.cumsum(pca_a.explained_variance_ratio_, axis=0)
top5 = numpy.where(cumulative_sum > 0.9)[0][0]

transformed_train_5 = transformed_train_a[:, 0:top5]
transformed_test_5 = transformed_test_a[:, 0:top5]


cumulative_sum = numpy.cumsum(pca_b.explained_variance_ratio_, axis=0)
top5 = numpy.where(cumulative_sum > 0.9)[0][0]

transformed_train_b5 = transformed_train_b[:, 0:top5]
transformed_test_b5 = transformed_test_b[:, 0:top5]

"""# Part 2"""

def GetSiameseData(imgs, labels, batch_size,imgs_b):

    image_a = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    image_b = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    label = numpy.zeros(batch_size);
    
    for i in range(batch_size):
        
        if (i % 2 == 0):
            idx1 = random.randint(0, len(imgs) - 1)
            idx2 = random.randint(0, len(imgs) - 1)
            l = 1
            while (labels[idx1] != labels[idx2]):
                idx2 = random.randint(0, len(imgs) - 1)            
                
        else:
            idx1 = random.randint(0, len(imgs) - 1)
            idx2 = random.randint(0, len(imgs) - 1)
            l = 0
            while (labels[idx1] == labels[idx2]):
                idx2 = random.randint(0, len(imgs) - 1)

        image_a[i, :, :, :] = imgs[idx1,:,:,:]
        image_b[i, :, :, :] = imgs_b[idx2,:,:,:]
        label[i] = l
#Genarate pair data base on batch_size
    return [image_a, image_b], label

def GetTripletData(imgs, labels, batch_size):

    image_a = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    image_b = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    image_c = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    
    for i in range(batch_size):
        
        idx1 = random.randint(0, len(imgs) - 1)
        idx2 = random.randint(0, len(imgs) - 1)
        idx3 = random.randint(0, len(imgs) - 1)

        while (labels[idx1] != labels[idx2]):
            idx2 = random.randint(0, len(imgs) - 1)            
                
        while (labels[idx1] == labels[idx3]):
            idx3 = random.randint(0, len(imgs) - 1)

        image_a[i, :, :, :] = imgs[idx1,:,:,:]
        image_b[i, :, :, :] = imgs[idx2,:,:,:]
        image_c[i, :, :, :] = imgs[idx3,:,:,:]

    return [image_a, image_b, image_c]

def TripleGenerator(imgs, labels, batch_size):
    while True:
        [image_a, image_b, image_c] = GetTripletData(imgs, labels, batch_size)
        yield [image_a, image_b, image_c], None
        
test = TripleGenerator(test_X, test_Y, 10)
x, _ = next(test)

fig = plt.figure(figsize=[25, 10])
#plot the graph using yield python genarator set the title 
for i in range(9):
    ax = fig.add_subplot(3, 9, i*3 + 1)
    ax.imshow(x[0][i,:,:,0])
    ax.set_title('Triple ' + str(i) + ': Anchor')
    
    ax = fig.add_subplot(3, 9, i*3 + 2)
    ax.imshow(x[1][i,:,:,0])    
    ax.set_title('Triple ' + str(i) + ': Positive')

    ax = fig.add_subplot(3, 9, i*3 + 3)
    ax.imshow(x[2][i,:,:,0])    
    ax.set_title('Triple ' + str(i) + ': Negative')

def conv_block(inputs, filters, spatial_dropout = 0.0, max_pool = True):
    
    x = layers.Conv2D(filters=filters, kernel_size=(3,3), padding='same', activation='relu')(inputs)
    x = layers.Conv2D(filters=filters, kernel_size=(3,3), padding='same', activation=None)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    if (spatial_dropout > 0.0):
        x = layers.SpatialDropout2D(spatial_dropout)(x)
    if (max_pool == True):
        x = layers.MaxPool2D(pool_size=(2, 2))(x)
    
    return x

def fc_block(inputs, size, dropout):
    x = layers.Dense(size, activation=None)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    if (dropout > 0.0):
        x = layers.Dropout(dropout)(x)
    
    return x

def vgg_net(inputs, filters, fc, spatial_dropout = 0.0, dropout = 0.0):
    
    x = inputs
    for idx,i in enumerate(filters):
        x = conv_block(x, i, spatial_dropout, not (idx==len(filters) - 1))
    
    x = layers.Flatten()(x)
    
    for i in fc:
        x = fc_block(x, i, dropout)
        
    return x

embedding_size = 32
dummy_input = keras.Input((128, 48, 3))
base_network = vgg_net(dummy_input, [8, 16, 32], [256], 0.2, 0)
embedding_layer = layers.Dense(embedding_size, activation=None)(base_network)
base_network = keras.Model(dummy_input, embedding_layer, name='SiameseBranch')

class TripletLossLayer(layers.Layer):
    def __init__(self, alpha, **kwargs):
        self.alpha = alpha
        super(TripletLossLayer, self).__init__(**kwargs)
    
    def triplet_loss(self, inputs):
        anchor, positive, negative = inputs
        
        anchor = K.l2_normalize(anchor, axis=1)
        positive = K.l2_normalize(positive, axis=1)
        negative = K.l2_normalize(negative, axis=1)

        p_dist = K.sum(K.square(anchor-positive), axis=-1)
        n_dist = K.sum(K.square(anchor-negative), axis=-1)
        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)
    
    def call(self, inputs):
        loss = self.triplet_loss(inputs)
        self.add_loss(loss)
        return loss
    
input_anchor = keras.Input((128, 48, 3), name='Anchor')
input_positive = keras.Input((128, 48, 3), name='Positive')
input_negative = keras.Input((128, 48, 3), name='Negative')

embedding_anchor = base_network(input_anchor)
embedding_positive = base_network(input_positive)
embedding_negative = base_network(input_negative)

margin = 1
loss_layer = TripletLossLayer(alpha=margin, name='triplet_loss_layer')([embedding_anchor, embedding_positive, embedding_negative])

triplet_network = keras.Model(inputs=[input_anchor, input_positive, input_negative], outputs=loss_layer)
triplet_network.summary()

triplet_network.compile(optimizer=keras.optimizers.RMSprop())

batch_size = 256
training_gen = TripleGenerator(train_X, train_Y, batch_size)
triplet_test_x = GetTripletData(test_X, test_Y, batch_size)

triplet_network.fit(training_gen, steps_per_epoch = 20, epochs=10, validation_data=(triplet_test_x, None))

def ComputeDistance(x, y):
    x = K.l2_normalize(x, axis=1)
    y = K.l2_normalize(y, axis=1)
    dist = K.sum(K.square(x - y), axis=-1)
    return dist

x = GetTripletData(test_X, test_Y, 9)
anchor_embedding = base_network.predict(x[0])
positive_embedding = base_network.predict(x[1])
negative_embedding = base_network.predict(x[2])
pos_dists = ComputeDistance(anchor_embedding, positive_embedding)
neg_dists = ComputeDistance(anchor_embedding, negative_embedding)

fig = plt.figure(figsize=[25, 9])
for i in range(9):
    ax = fig.add_subplot(3, 9, i*3 + 1)
    ax.imshow(x[0][i,:,:,0])
    ax.set_title('Anchor ' + str(i))
    
    ax = fig.add_subplot(3, 9, i*3 + 2)
    ax.imshow(x[1][i,:,:,0])    
    ax.set_title('Pos: ' + '%1.2f' % float(pos_dists[i]))
    
    ax = fig.add_subplot(3, 9, i*3 + 3)
    ax.imshow(x[2][i,:,:,0])    
    ax.set_title('Neg: ' + '%1.2f' % float(neg_dists[i]))

embeddings = base_network.predict(train_X)
tsne_embeddings = TSNE(random_state=4).fit_transform(embeddings)
fig = plt.figure(figsize=[12, 12])
ax = fig.add_subplot(1, 1, 1)
ax.scatter(tsne_embeddings[:,0], tsne_embeddings[:,1], c = train_Y.flatten());

