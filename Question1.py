# -*- coding: utf-8 -*-
"""Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1niBRKXab_9x1fCmDsAqyRql3t1rJQvXa
"""

import random
import numpy
import numpy.matlib
import scipy
import matplotlib.pyplot as plt

import os
import glob
import cv2

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import backend as K

from sklearn import decomposition
from sklearn import discriminant_analysis
from sklearn import datasets
from sklearn.manifold import TSNE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import plot_confusion_matrix

test_a = '/content/drive/My Drive/Colab Notebooks/VIPeR/Test/cam_a/*.bmp'
test_b = '/content/drive/My Drive/Colab Notebooks/VIPeR/Test/cam_b/*.bmp'
train_a = "/content/drive/My Drive/Colab Notebooks/VIPeR/Train/cam_a/*.bmp"
train_b = "/content/drive/My Drive/Colab Notebooks/VIPeR/Train/cam_b/*.bmp"

files = glob.glob(test_a)
files.sort()

data_test_a = []
for f in files:
    d = {}
    head, tail = os.path.split(f)
    parts = tail.split('_')
    if (len(parts) == 2):
        d['label'] = int(parts[0])
        d['image'] = cv2.imread(f)
        data_test_a.append(d)
    else:
        print('Could not load: ' + f + '! Incorrectly formatted filename')

files = glob.glob(test_b)
files.sort()

data_test_b = []
for f in files:
    d = {}
    head, tail = os.path.split(f)
    parts = tail.split('_')
    if (len(parts) == 2):
        d['label'] = int(parts[0])
        d['image'] = cv2.imread(f)
        data_test_b.append(d)
    else:
        print('Could not load: ' + f + '! Incorrectly formatted filename')

files = glob.glob(train_a)
files.sort()

data_train_a = []
for f in files:
    d = {}
    head, tail = os.path.split(f)
    parts = tail.split('_')
    if (len(parts) == 2):
        d['label'] = int(parts[0])
        d['image'] = cv2.imread(f)
        data_train_a.append(d)
    else:
        print('Could not load: ' + f + '! Incorrectly formatted filename')

files = glob.glob(train_b)
files.sort()

#data_train_b = []
for f in files:
    d = {}
    head, tail = os.path.split(f)
    parts = tail.split('_')
    if (len(parts) == 2):
        d['label'] = int(parts[0])
        d['image'] = cv2.imread(f)
        data_train_a.append(d)
    else:
        print('Could not load: ' + f + '! Incorrectly formatted filename')

test_a_X = numpy.array([d['image'] for d in data_test_a])/255
test_a_Y = numpy.array([d['label'] for d in data_test_a])

test_b_X = numpy.array([d['image'] for d in data_test_b])/255
test_b_Y = numpy.array([d['label'] for d in data_test_b])

train_a_X = numpy.array([d['image'] for d in data_train_a])/255
train_a_Y = numpy.array([d['label'] for d in data_train_a])

#train_b_X = numpy.array([d['image'] for d in data_train_b])/255
#train_b_Y = numpy.array([d['label'] for d in data_train_b])

fig = plt.figure(figsize=[10, 8])
for i in range(10):
    ax = fig.add_subplot(4, 10, i + 1)
    ax.imshow(test_a_X[i,:,:,0])
for i in range(10):
    ax = fig.add_subplot(4, 10, i + 11)
    ax.imshow(test_b_X[i,:,:,0]) 
for i in range(10):
    ax = fig.add_subplot(4, 10, i + 21)
    ax.imshow(train_a_X[i,:,:,0]) 
# for i in range(10):
#     ax = fig.add_subplot(4, 10, i + 31)
#     ax.imshow(train_b_X[i,:,:,0])

"""# Part 1"""

nsamples, nx, ny, nz = train_a_X.shape
train_a_X_reshape = train_a_X.reshape((nsamples,nx*ny*nz))

# nsamples, nx, ny, nz = train_b_X.shape
# train_b_X_reshape = train_b_X.reshape((nsamples,nx*ny*nz))

nsamples, nx, ny, nz = test_a_X.shape
test_a_X_reshape = test_a_X.reshape((nsamples,nx*ny*nz))

nsamples, nx, ny, nz = test_b_X.shape
test_b_X_reshape = test_b_X.reshape((nsamples,nx*ny*nz))

pca = decomposition.PCA()
pca.fit(train_a_X_reshape)
transformed = pca.transform(test_a_X_reshape)
transformed_test = pca.transform(test_b_X_reshape)

cumulative_sum = numpy.cumsum(pca.explained_variance_ratio_, axis=0)
top1 = numpy.where(cumulative_sum > 0.99)[0][0]
top5 = numpy.where(cumulative_sum > 0.95)[0][0]
top10 = numpy.where(cumulative_sum > 0.9)[0][0]


transformed_train_1 = transformed[:, 0:top1]
transformed_test_1 = transformed_test[:, 0:top1]
transformed_train_5 = transformed[:, 0:top5]
transformed_test_5 = transformed_test[:, 0:top5]
transformed_train_10 = transformed[:, 0:top10]
transformed_test_10 = transformed_test[:, 0:top10]

def eval_model(model, X_train, Y_train, X_test, Y_test):
    fig = plt.figure(figsize=[25, 8])
    ax = fig.add_subplot(1, 2, 1)
    conf = plot_confusion_matrix(model, X_train, Y_train, normalize='true', ax=ax)
    conf.ax_.set_title('Training Set Performance');
    ax = fig.add_subplot(1, 2, 2)
    conf = plot_confusion_matrix(model, X_test, Y_test, normalize='true', ax=ax)
    conf.ax_.set_title('Test Set Performance');
    pred = model.predict(X_test)
    print('Test Accuracy: ' + str(sum(pred == Y_test)/len(Y_test)))

cknn_95 = KNeighborsClassifier(n_neighbors=10, weights='distance')
cknn_95.fit(transformed_train_10, test_a_Y)
eval_model(cknn_95, transformed_train_10, test_a_Y, transformed_test_10, test_b_Y)

"""# Part 2"""

def GetSiameseData(imgs, labels, batch_size):

    image_a = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    image_b = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    label = numpy.zeros(batch_size);
    
    for i in range(batch_size):
        
        if (i % 2 == 0):
            idx1 = random.randint(0, len(imgs) - 1)
            idx2 = random.randint(0, len(imgs) - 1)
            l = 1
            while (labels[idx1] != labels[idx2]):
                idx2 = random.randint(0, len(imgs) - 1)            
                
        else:
            idx1 = random.randint(0, len(imgs) - 1)
            idx2 = random.randint(0, len(imgs) - 1)
            l = 0
            while (labels[idx1] == labels[idx2]):
                idx2 = random.randint(0, len(imgs) - 1)

        image_a[i, :, :, :] = imgs[idx1,:,:,:]
        image_b[i, :, :, :] = imgs[idx2,:,:,:]
        label[i] = l

    return [image_a, image_b], label

def GetTripletData(imgs, labels, batch_size):

    image_a = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    image_b = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    image_c = numpy.zeros((batch_size, numpy.shape(imgs)[1], numpy.shape(imgs)[2], numpy.shape(imgs)[3]));
    
    for i in range(batch_size):
        
        idx1 = random.randint(0, len(imgs) - 1)
        idx2 = random.randint(0, len(imgs) - 1)
        idx3 = random.randint(0, len(imgs) - 1)

        while (labels[idx1] != labels[idx2]):
            idx2 = random.randint(0, len(imgs) - 1)            
                
        while (labels[idx1] == labels[idx3]):
            idx3 = random.randint(0, len(imgs) - 1)

        image_a[i, :, :, :] = imgs[idx1,:,:,:]
        image_b[i, :, :, :] = imgs[idx2,:,:,:]
        image_c[i, :, :, :] = imgs[idx3,:,:,:]

    return [image_a, image_b, image_c]

def TripleGenerator(imgs, labels, batch_size):
    while True:
        [image_a, image_b, image_c] = GetTripletData(imgs, labels, batch_size)
        yield [image_a, image_b, image_c], None
        
test = TripleGenerator(test_a_X, test_a_Y, 9)
x, _ = next(test)

fig = plt.figure(figsize=[25, 10])
for i in range(9):
    ax = fig.add_subplot(3, 9, i*3 + 1)
    ax.imshow(x[0][i,:,:,0])
    ax.set_title('Triple ' + str(i) + ': Anchor')
    
    ax = fig.add_subplot(3, 9, i*3 + 2)
    ax.imshow(x[1][i,:,:,0])    
    ax.set_title('Triple ' + str(i) + ': Positive')

    ax = fig.add_subplot(3, 9, i*3 + 3)
    ax.imshow(x[2][i,:,:,0])    
    ax.set_title('Triple ' + str(i) + ': Negative')

def conv_block(inputs, filters, spatial_dropout = 0.0, max_pool = True):
    
    x = layers.Conv2D(filters=filters, kernel_size=(3,3), padding='same', activation='relu')(inputs)
    x = layers.Conv2D(filters=filters, kernel_size=(3,3), padding='same', activation=None)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    if (spatial_dropout > 0.0):
        x = layers.SpatialDropout2D(spatial_dropout)(x)
    if (max_pool == True):
        x = layers.MaxPool2D(pool_size=(2, 2))(x)
    
    return x

def fc_block(inputs, size, dropout):
    x = layers.Dense(size, activation=None)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    if (dropout > 0.0):
        x = layers.Dropout(dropout)(x)
    
    return x

def vgg_net(inputs, filters, fc, spatial_dropout = 0.0, dropout = 0.0):
    
    x = inputs
    for idx,i in enumerate(filters):
        x = conv_block(x, i, spatial_dropout, not (idx==len(filters) - 1))
    
    x = layers.Flatten()(x)
    
    for i in fc:
        x = fc_block(x, i, dropout)
        
    return x

embedding_size = 32
dummy_input = keras.Input((32, 32, 1))
base_network = vgg_net(dummy_input, [8, 16, 32], [256], 0.2, 0)
embedding_layer = layers.Dense(embedding_size, activation=None)(base_network)
base_network = keras.Model(dummy_input, embedding_layer, name='SiameseBranch')

class TripletLossLayer(layers.Layer):
    def __init__(self, alpha, **kwargs):
        self.alpha = alpha
        super(TripletLossLayer, self).__init__(**kwargs)
    
    def triplet_loss(self, inputs):
        anchor, positive, negative = inputs
        
        anchor = K.l2_normalize(anchor, axis=1)
        positive = K.l2_normalize(positive, axis=1)
        negative = K.l2_normalize(negative, axis=1)

        p_dist = K.sum(K.square(anchor-positive), axis=-1)
        n_dist = K.sum(K.square(anchor-negative), axis=-1)
        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)
    
    def call(self, inputs):
        loss = self.triplet_loss(inputs)
        self.add_loss(loss)
        return loss
    
input_anchor = keras.Input((32, 32, 1), name='Anchor')
input_positive = keras.Input((32, 32, 1), name='Positive')
input_negative = keras.Input((32, 32, 1), name='Negative')

embedding_anchor = base_network(input_anchor)
embedding_positive = base_network(input_positive)
embedding_negative = base_network(input_negative)

margin = 1
loss_layer = TripletLossLayer(alpha=margin, name='triplet_loss_layer')([embedding_anchor, embedding_positive, embedding_negative])

triplet_network = keras.Model(inputs=[input_anchor, input_positive, input_negative], outputs=loss_layer)
triplet_network.summary()

triplet_network.compile(optimizer=keras.optimizers.RMSprop())

batch_size = 20
training_gen = TripleGenerator(test_a_X, test_a_Y, batch_size)
triplet_test_x = GetTripletData(test_b_X, test_b_Y, 1000)

triplet_network.fit(training_gen, steps_per_epoch = 20, epochs=20, validation_data=(triplet_test_x, None))